{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f11c82b",
   "metadata": {},
   "source": [
    "![image](https://tech.gmogshd.com/wp-content/uploads/2023/03/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2023-03-17-0.06.54.png)\n",
    "\n",
    "# transfomer\n",
    "\n",
    "## Feature\n",
    "\n",
    "- encoderとdecoderが積み重なっている\n",
    "- 文章を翻訳時encoderで文章を入力\n",
    "- decoderで過去の文章を繰り返し学習する。encoderで入力があった時学習したモデルと照らし合わせて一番似ている文字を出力する\n",
    "\n",
    "## Implementation\n",
    "- tensorflowのtutorialを通してattentionを実装する\n",
    "\n",
    "## ⚠️Note\n",
    "- P100GPU1基で実行しても10分ほどかかる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55875b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7be49e",
   "metadata": {},
   "source": [
    "データセットをダウンロードしたあと、データを準備するために下記のようないくつかの手順を実行します。\n",
    "\n",
    "- それぞれの文ごとに、開始 と 終了 のトークンを付加する\n",
    "- 特殊文字を除去して文をきれいにする\n",
    "- 単語インデックスと逆単語インデックス（単語 → id と id → 単語のマッピングを行うディクショナリ）を作成する\n",
    "- 最大長にあわせて各文をパディングする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889e86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_zip = tf.keras.utils.get_file(\n",
    "#     fname='spa-eng.zip', \n",
    "#     origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "#     extract=True\n",
    "#     )\n",
    "\n",
    "path_to_file = \"../../data/spa-eng/spa.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6893b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 単語とその後の句読点の間にスペースを挿入\n",
    "    # e.g. \"he is a boy.\" => \"he is a boy .\"\n",
    "    # 参照：- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]', \" \", w)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \".\")以外のすべての記号をスペースに置き換え\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # 文の開始と終了のトークンを付与\n",
    "    # モデルが予測をいつ開始し、終了すればいいかを知らせるため\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "785edfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "# 事前処理を見てみる\n",
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))\n",
    "\n",
    "# print(b\"\\xc2\\xbf\".decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b55e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    \"\"\"\n",
    "    firstly, create_dataset removes accent sign, then clean the sentence\n",
    "    return: pair of [ENGLISH, SPANISH]\n",
    "    \"\"\"\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split(\"\\n\")\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split(\"\\t\")] for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d17ebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca198d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa872237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters=''\n",
    "    )\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.text_to_sequence(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tensor,\n",
    "        padding='post'\n",
    "    )\n",
    "\n",
    "    return tensor, lang_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36605b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03a920",
   "metadata": {},
   "source": [
    "実験を早くするためにデータセットのサイズを制限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f4bc859",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'text_to_sequence'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# このサイズのデータセットで実験\u001b[39;00m\n\u001b[32m      2\u001b[39m num_examples = \u001b[32m30000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m input_tensor, target_tensor, inp_lang, targ_lang = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ターゲットテンソルの最大長を計算\u001b[39;00m\n\u001b[32m      6\u001b[39m max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, num_examples)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_dataset\u001b[39m(path, num_examples=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      2\u001b[39m     targ_lang, inp_lang = create_dataset(path, num_examples)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     input_tensor, inp_lang_tokenizer = \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m      2\u001b[39m lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n\u001b[32m      3\u001b[39m     filters=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m )\n\u001b[32m      5\u001b[39m lang_tokenizer.fit_on_texts(lang)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tensor = \u001b[43mlang_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_to_sequence\u001b[49m(lang)\n\u001b[32m      8\u001b[39m tensor = tf.keras.preprocessing.sequence.pad_sequences(\n\u001b[32m      9\u001b[39m     tensor,\n\u001b[32m     10\u001b[39m     padding=\u001b[33m'\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor, lang_tokenizer\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tokenizer' object has no attribute 'text_to_sequence'"
     ]
    }
   ],
   "source": [
    "# このサイズのデータセットで実験\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# ターゲットテンソルの最大長を計算\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147fce90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 80-20で分割を行い、訓練用と検証用のデータセットを作成\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\u001b[43minput_tensor\u001b[49m, target_tensor, test_size=\u001b[32m0.2\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 長さを表示\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_tensor_train), \u001b[38;5;28mlen\u001b[39m(target_tensor_train), \u001b[38;5;28mlen\u001b[39m(input_tensor_val), \u001b[38;5;28mlen\u001b[39m(target_tensor_val))\n",
      "\u001b[31mNameError\u001b[39m: name 'input_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "# 80-20で分割を行い、訓練用と検証用のデータセットを作成\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# 長さを表示\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "223464b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04507463",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e847ee",
   "metadata": {},
   "source": [
    "tf.dataデータセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b87aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab0f96",
   "metadata": {},
   "source": [
    "TODO\n",
    "- エンコーダー, デコーダーを記述\n",
    "- 翻訳を試す"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
